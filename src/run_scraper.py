import asyncio
import logging
import pandas as pd
import random
from playwright.async_api import async_playwright
from src.config import SEARCH_KEYWORDS, CSV_MAIN, CSV_RAW_DAILY
from src.parser import get_vacancy_links
from src.scraper import get_vacancy_details
from src.utils import setup_logger, save_to_main_csv, load_existing_links, save_raw_data
from src.utils import canonical_link  # üü¢ –∏–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ query

MAX_CONCURRENT_TASKS = 10

async def scrape_single(link, semaphore, context, results, idx, total):
    async with semaphore:
        try:
            page = await context.new_page()
            logging.info(f"[{idx}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link}")
            data = await get_vacancy_details(link, page)
            await page.close()
            if data:
                results.append(data)

            delay = random.uniform(1, 2)
            logging.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º: {delay:.2f} —Å–µ–∫.")
            await asyncio.sleep(delay)

        except Exception as e:
            logging.warning(f"[Scrape Error] {link}: {e}")

async def run_scraper(mode: str = "daily") -> pd.DataFrame:
    setup_logger()

    # üìå –í–º–µ—Å—Ç–æ ¬´—Å—ã—Ä—ã—Ö¬ª —Å—Å—ã–ª–æ–∫ ‚Äî —Å—Ä–∞–∑—É –±–µ—Ä—ë–º —É–∂–µ –∫–∞–Ω–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    existing_links = {
        canonical_link(l)            # üü¢ –æ–±—Ä–µ–∑–∞–µ–º –≤—Å—ë –ø–æ—Å–ª–µ '?'
        for l in load_existing_links(CSV_MAIN)
    }

    logging.info(f"üîç –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞: {mode.upper()}")
    logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Å—ã–ª–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")

    all_links = set()
    for keyword in SEARCH_KEYWORDS:
        max_pages = 100 if mode == "full" else 1
        raw_links = await get_vacancy_links(keyword, max_pages=max_pages)
        for raw in raw_links:
            # üü¢ —Å—Ä–∞–∑—É –∫–∞–Ω–æ–Ω–∏–∑–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ query-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            all_links.add(canonical_link(raw))

    # üìå –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É —É–∂–µ –ø–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º URL
    new_links = list(all_links - existing_links)
    logging.info(f"üîó –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_links)}")

    results = []
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36"
            ),
            locale="ru-RU"
        )

        # new_links —É–∂–µ —á–∏—Å—Ç—ã–µ URL ‚Äî –º–æ–∂–Ω–æ –ø—Ä—è–º–æ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å
        tasks = [
            scrape_single(link, semaphore, context, results, idx, len(new_links))
            for idx, link in enumerate(new_links, 1)
        ]

        await asyncio.gather(*tasks)
        await browser.close()

    results = [r for r in results if r is not None]

    if results:
        df = pd.DataFrame(results)
        save_to_main_csv(results, CSV_MAIN)  # –∑–¥–µ—Å—å —Ç–æ–∂–µ util-—Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∫–∞–Ω–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        save_raw_data(df, CSV_RAW_DAILY)
        logging.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π")
        return df
    else:
        logging.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return pd.DataFrame()
