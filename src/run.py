import asyncio
from src.config import SEARCH_KEYWORDS, CSV_PATH
from src.parser import get_vacancy_links
from src.scraper import get_vacancy_details
from src.utils import setup_logger, save_to_csv, load_existing_links
import logging

async def main():
    setup_logger()
    all_links = set()
    existing_links = load_existing_links(CSV_PATH)

    logging.info("üìå –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")

    for keyword in SEARCH_KEYWORDS:
        links = await get_vacancy_links(keyword, max_pages=10)
        all_links.update(links)

    new_links = list(set(all_links) - existing_links)
    logging.info(f"üÜï –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_links)}")

    all_data = []
    for i, link in enumerate(new_links, 1):
        logging.info(f"üìÑ [{i}/{len(new_links)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link}")
        try:
            vacancy_data = await get_vacancy_details(link)
            all_data.append(vacancy_data)
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")

    if all_data:
        save_to_csv(all_data, CSV_PATH)
        logging.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_data)} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ {CSV_PATH}")
    else:
        logging.info("‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

if __name__ == "__main__":
    asyncio.run(main())
